# -*- coding: utf-8 -*-
"""3LLMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ZRh4bMPbVnutlRU6Mfu4KNJL6Ju0KfS
"""

!pip install -U bitsandbytes

from huggingface_hub import notebook_login
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel, PeftConfig
from tqdm.notebook import tqdm
hf_token = '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'
from huggingface_hub import login
import os
import pandas as pd
from peft import PeftModel, PeftConfig
login(token=hf_token)

model_name = "Likich/falcon-finetune-qualcoding_1000_prompt1_dot"

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

PEFT_MODEL = model_name
config = PeftConfig.from_pretrained(PEFT_MODEL)
falcon = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    output_hidden_states=True,  # Enable hidden state outputs
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    token=hf_token
)

tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, token= hf_token)
tokenizer.pad_token = tokenizer.eos_token

device = "cuda:0"
# Configure generation settings
generation_config = falcon.generation_config
generation_config.max_new_tokens = 15
generation_config.temperature = 0.7
generation_config.top_p = 0.7
generation_config.num_return_sequences = 1
generation_config.pad_token_id = tokenizer.eos_token_id
generation_config.eos_token_id = tokenizer.eos_token_id

falcon.to(device)

model_name1 = "Likich/llama3-finetune-qualcoding_1000_prompt1_dot"
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

PEFT_MODEL1 = model_name1
config = PeftConfig.from_pretrained(PEFT_MODEL1)
llama = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    output_hidden_states=True,  # Enable hidden state outputs
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    token=hf_token
)

tokenizer1 = AutoTokenizer.from_pretrained(config.base_model_name_or_path, token= hf_token)
tokenizer1.pad_token = tokenizer1.eos_token

device = "cuda:0"
# Configure generation settings
generation_config = llama.generation_config
generation_config.max_new_tokens = 15
generation_config.temperature = 0.7
generation_config.top_p = 0.7
generation_config.num_return_sequences = 1
generation_config.pad_token_id = tokenizer.eos_token_id
generation_config.eos_token_id = tokenizer.eos_token_id

llama.to(device)

model_name2 = "Likich/mistral-finetune-qualcoding_1000_prompt2"
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

PEFT_MODEL2 = model_name2
config = PeftConfig.from_pretrained(PEFT_MODEL2)
mistral = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    output_hidden_states=True,  # Enable hidden state outputs
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    token=hf_token
)

tokenizer2 = AutoTokenizer.from_pretrained(config.base_model_name_or_path, token= hf_token)
tokenizer2.pad_token = tokenizer2.eos_token

device = "cuda:0"
# Configure generation settings
generation_config = mistral.generation_config
generation_config.max_new_tokens = 15
generation_config.temperature = 0.7
generation_config.top_p = 0.7
generation_config.num_return_sequences = 1
generation_config.pad_token_id = tokenizer2.eos_token_id
generation_config.eos_token_id = tokenizer2.eos_token_id

mistral.to(device)

# Functions simulating the three LLMs' responses (replace with actual LLM calls)
def llama3_generate_code(sentence):
    prompt = f"<human>: Can you tell me what the main idea of this sentence is in just a few words? Output only the main idea, without introductory words.{sentence}\n<assistant>:".strip()
    encoding_llama = tokenizer1(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = llama(
            input_ids=encoding_llama.input_ids,
            attention_mask=encoding_llama.attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )
        hidden_states = outputs.hidden_states  # List of tensors, one per layer
    with torch.no_grad():
        outputs = llama.generate(input_ids=encoding_llama.input_ids, attention_mask=encoding_llama.attention_mask, generation_config=generation_config)
        parts = tokenizer1.decode(outputs[0]).split("<assistant>:")
        summary = parts[1].split("\n")[0].strip()
    return summary

def falcon_generate_code(sentence):
    prompt = f"<human>: Summarize the main idea of a sentence.{sentence}\n<assistant>:".strip()
    encoding_falcon = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = falcon(
            input_ids=encoding_falcon.input_ids,
            attention_mask=encoding_falcon.attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )
        hidden_states = outputs.hidden_states  # List of tensors, one per layer
    with torch.no_grad():
        outputs = falcon.generate(input_ids=encoding_falcon.input_ids, attention_mask=encoding_falcon.attention_mask, generation_config=generation_config)
        parts = tokenizer.decode(outputs[0]).split("<assistant>:")
        summary = parts[1].split("\n")[0].strip()
    return summary

def mistral_generate_code(sentence):
    prompt = f"<human>: Can you tell me what the main idea of this sentence is in just a few words? {sentence}\n<assistant>:".strip()
    encoding_mistral = tokenizer2(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = mistral(
            input_ids=encoding_mistral.input_ids,
            attention_mask=encoding_mistral.attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )
        hidden_states = outputs.hidden_states  # List of tensors, one per layer
    with torch.no_grad():
        outputs = mistral.generate(input_ids=encoding_mistral.input_ids, attention_mask=encoding_mistral.attention_mask, generation_config=generation_config)
        parts = tokenizer2.decode(outputs[0]).split("<assistant>:")
        summary = parts[1].split("\n")[0].strip()
    return summary    # Simulated code generation

# File path for the transcript
file_path = "text.txt"

# Read the transcript file
with open(file_path, "r", encoding="utf-8") as file:
    transcript = file.read()

# Split the transcript into paragraphs (non-empty lines)
paragraphs = [para.strip() for para in transcript.split("\n") if para.strip()]

# Prepare data for the DataFrame
data = []
for paragraph in paragraphs:
    data.append({
            "Paragraph": paragraph,
            "Llama3_Code": llama3_generate_code(paragraph),
            "Falcon_Code": falcon_generate_code(paragraph),
            "Mistral_Code": mistral_generate_code(paragraph),
        })

# Create a DataFrame
df = pd.DataFrame(data)

# Save the DataFrame to a CSV file
output_path = "coded_transcript.csv"
df.to_csv(output_path, index=False)

print(f"Processed transcript saved to: {output_path}")



## Knowledge fusion

"""## Knowledge fusion"""

!pip install openai==0.28

import pandas as pd
import random
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import openai  # Install the OpenAI library if not already installed

textdescription = 'Interview about interaction of users with voice assistants.'
# Load the table
df = pd.read_csv('coded_transcript (1).csv')  # Replace with your actual CSV file name

# Define similarity threshold
SIMILARITY_THRESHOLD = 0.8

# Define weights for the models
WEIGHTS = {'Llama3_Code': 0.4, 'Falcon_Code': 0.4, 'Mistral_Code': 0.2}

# Function to calculate token overlap similarity
def calculate_similarity(code1, code2):
    vectorizer = CountVectorizer().fit_transform([code1, code2])
    vectors = vectorizer.toarray()
    return cosine_similarity(vectors)[0, 1]

# Function to decide the final code
def decide_final_code(row):
    codes = [row['Llama3_Code'], row['Falcon_Code'], row['Mistral_Code']]

    # Calculate pairwise similarities
    sim_1_2 = calculate_similarity(codes[0], codes[1])
    sim_1_3 = calculate_similarity(codes[0], codes[2])
    sim_2_3 = calculate_similarity(codes[1], codes[2])

    # If similarities are high, choose randomly
    if sim_1_2 > SIMILARITY_THRESHOLD and sim_1_3 > SIMILARITY_THRESHOLD and sim_2_3 > SIMILARITY_THRESHOLD:
        return random.choice(codes)

    # Otherwise, use GPT to make the decision
    else:
        # Prepare the input prompt for GPT
        prompt = f"""
        You will be given a paragraph from the text, which is: {textdescription}.

        Definition of the code: A word or short phrase that symbolically assigns a summative, salient, essence-capturing, and/or evocative attribute for a portion of language-based or visual data.

        Here is the exerpt to code:
        {row['Paragraph']}

        Here are three coding suggestions from previous models:
        1. {row['Llama3_Code']}
        2. {row['Falcon_Code']}
        3. {row['Mistral_Code']}

        Please suggest a code taking into account all these answers.
        Output should be the code with no longer than 5 words.
        """
        # Call GPT chat-based API (replace with your API key)
        openai.api_key = ''
        response = openai.ChatCompletion.create(
            model="gpt-4o",  # Replace with the desired GPT model
            messages=[
                {"role": "system", "content": "You are a coding assistant skilled in thematic analysis."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200
        )

        # Extract the final decision from GPT's response
        return response['choices'][0]['message']['content'].strip()

# Apply the function to the dataset
df['Final_Code'] = df.apply(decide_final_code, axis=1)

# Save the updated table
df.to_csv('final_coded_transcript.csv', index=False)

print("Final codes have been decided and saved to 'final_coded_transcript.csv'.")

import pandas as pd
import re

# Function to clean garbage text
def clean_code_text(text):
    if not isinstance(text, str):  # Ensure text is a string
        return text
    # Remove tags like <...> or </...>
    cleaned_text = re.sub(r'<[^>]*>', '', text)  # Remove anything within <>
    # Remove common garbage words or patterns
    garbage_patterns = [
        r'</',        # Closing tags
        r'assistant', # "assistant" keyword
        r'<h',        # Tags starting with <h
        r'leot_id',   # "leot_id" keyword
        r'start_header_id',  # "start_header_id" keyword
        r'end_header_id',    # "end_header_id" keyword
    ]
    for pattern in garbage_patterns:
        cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
    # Normalize spaces and strip leading/trailing spaces
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

# Load the CSV file
df = pd.read_csv('final_coded_transcript.csv')

# Clean all code columns
code_columns = ['Llama3_Code', 'Falcon_Code', 'Mistral_Code', 'Final_Code']  # List your code columns
for column in code_columns:
    df[column] = df[column].apply(clean_code_text)

# Save the cleaned data back to a CSV
df.to_csv('cleaned_coded_transcript.csv', index=False)

print("Garbage text has been thoroughly cleaned and saved to 'cleaned_coded_transcript.csv'.")

train = pd.read_csv('/content/training_data_1000.csv')

df

goldstandard=train[100:182]

goldstandard.rename(columns={'Prompt': 'GD'}, inplace=True)

goldstandard.rename(columns={'User': 'Paragraph'}, inplace=True)

goldstandard

import pandas as pd


# Preprocess 'Paragraph' column in both DataFrames
def preprocess_text(text):
    return text.strip().lower().replace('\n', '').replace('\r', '')

df['Paragraph_processed'] = df['Paragraph'].apply(preprocess_text)
goldstandard['Paragraph_processed'] = goldstandard['Paragraph'].apply(preprocess_text)

# Merge on the preprocessed column
merged_df = pd.merge(df, goldstandard[['Paragraph_processed', 'GD']],
                     on='Paragraph_processed', how='left')

# Drop the helper column if unnecessary
merged_df.drop(columns=['Paragraph_processed'], inplace=True)

# Save the merged DataFrame
merged_df.to_csv('merged_with_gd_fixed.csv', index=False)

print("The GD column has been added and saved to 'merged_with_gd_fixed.csv'.")

merged_df.tail()

# Remove duplicates based on the 'Paragraph' column
df_cleaned = merged_df.drop_duplicates(subset='Paragraph', keep='first')

# Save the cleaned DataFrame to a new CSV file
df_cleaned.to_csv('cleaned_no_duplicates.csv', index=False)

print("Duplicates have been removed and the cleaned file has been saved as 'cleaned_no_duplicates.csv'.")

df_cleaned

df_cleaned = df_cleaned.reset_index(drop=True)

"""Here we compute BERTscore, and then compute BERTscore after RAG

## RAG
"""



from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Load the cleaned DataFrame
df = df_cleaned.copy()

# Reset index to ensure it is sequential
df = df.reset_index(drop=True)

# Initialize TF-IDF Vectorizer to encode the text
vectorizer = TfidfVectorizer()

# Generate TF-IDF embeddings for the paragraphs
tfidf_matrix = vectorizer.fit_transform(df['Paragraph'])

# Compute the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(tfidf_matrix)

# Define a similarity threshold (adjust based on the desired strictness)
similarity_threshold = 0.8

# Track changes in Final_Code
updates = []

# Function to assign a code based on similarity
def assign_similar_codes(row_idx, similarity_matrix, dataframe):
    original_code = dataframe.at[row_idx, 'Final_Code']
    similarities = similarity_matrix[row_idx]
    similar_indices = [i for i, score in enumerate(similarities) if score >= similarity_threshold and i != row_idx]

    # Look for a similar row's code
    for idx in similar_indices:
        if dataframe.at[idx, 'Final_Code']:
            new_code = dataframe.at[idx, 'Final_Code']
            if new_code != original_code:  # Update only if there's a change
                updates.append({
                    "Paragraph": dataframe.at[row_idx, 'Paragraph'],
                    "Original_Final_Code": original_code,
                    "Updated_Final_Code": new_code,
                    "Similar_Paragraph": dataframe.at[idx, 'Paragraph']
                })
            return new_code

    # Return the original code if no updates are made
    return original_code

# Apply the function to assign similar codes
df['Final_Code'] = [assign_similar_codes(idx, cosine_sim_matrix, df) for idx in range(len(df))]

# Convert updates to a DataFrame for review
updates_df = pd.DataFrame(updates)

# Save updates and final DataFrame
updates_df.to_csv('code_updates.csv', index=False)
df.to_csv('updated_with_similar_codes.csv', index=False)

# Print before and after updates for review
print("=== Updates Made ===")
print(updates_df)
print("\n=== Updated DataFrame Preview ===")
print(df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Load the cleaned DataFrame
df = df_cleaned.copy()

# Reset index to ensure it is sequential
df = df.reset_index(drop=True)

# Initialize Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate sentence embeddings for the paragraphs
embeddings = model.encode(df['Paragraph'].tolist(), convert_to_tensor=False)

# Compute the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(embeddings)

# Define a similarity threshold (adjust based on the desired strictness)
similarity_threshold = 0.4

# Track changes in Final_Code
updates = []

# Function to assign a code based on similarity
def assign_similar_codes(row_idx, similarity_matrix, dataframe):
    original_code = dataframe.at[row_idx, 'Final_Code']
    similarities = similarity_matrix[row_idx]
    similar_indices = [i for i, score in enumerate(similarities) if score >= similarity_threshold and i != row_idx]

    # Look for a similar row's code
    for idx in similar_indices:
        if dataframe.at[idx, 'Final_Code']:
            new_code = dataframe.at[idx, 'Final_Code']
            if new_code != original_code:  # Update only if there's a change
                updates.append({
                    "Paragraph": dataframe.at[row_idx, 'Paragraph'],
                    "Original_Final_Code": original_code,
                    "Updated_Final_Code": new_code,
                    "Similar_Paragraph": dataframe.at[idx, 'Paragraph']
                })
            return new_code

    # Return the original code if no updates are made
    return original_code

# Apply the function to assign similar codes
df['Final_Code'] = [assign_similar_codes(idx, cosine_sim_matrix, df) for idx in range(len(df))]

# Convert updates to a DataFrame for review
updates_df = pd.DataFrame(updates)

# Save updates and final DataFrame
updates_df.to_csv('code_updates.csv', index=False)
df.to_csv('updated_with_similar_codes.csv', index=False)

# Print before and after updates for review
print("=== Updates Made ===")
print(updates_df)
print("\n=== Updated DataFrame Preview ===")
print(df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Load the cleaned DataFrame
df = df_cleaned.copy()

# Reset index to ensure it is sequential
df = df.reset_index(drop=True)

# Initialize Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate sentence embeddings for the paragraphs
embeddings = model.encode(df['Paragraph'].tolist(), convert_to_tensor=False)

# Compute the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(embeddings)

# Define a similarity threshold (adjust based on the desired strictness)
similarity_threshold = 1

# Track changes in Final_Code
updates = []

# Function to assign a code based on similarity
def assign_similar_codes(row_idx, dataframe):
    similarities = cosine_sim_matrix[row_idx, :row_idx]  # Compare only with previous rows
    similar_indices = [i for i, score in enumerate(similarities) if score >= similarity_threshold]

    # If similar indices exist, assign the first matching Final_Code
    for idx in similar_indices:
        new_code = dataframe.at[idx, 'Final_Code']
        if new_code != dataframe.at[row_idx, 'Final_Code']:
            updates.append({
                "Paragraph": dataframe.at[row_idx, 'Paragraph'],
                "Original_Final_Code": dataframe.at[row_idx, 'Final_Code'],
                "Updated_Final_Code": new_code,
                "Similar_Paragraph": dataframe.at[idx, 'Paragraph']
            })
            return new_code

    # Otherwise, keep the original code
    return dataframe.at[row_idx, 'Final_Code']

# Apply the function to assign similar codes
df['Final_Code'] = [
    assign_similar_codes(idx, df) for idx in range(len(df))
]

# Convert updates to a DataFrame for review
updates_df = pd.DataFrame(updates)

# Save updates and final DataFrame
updates_df.to_csv('code_updates.csv', index=False)
df.to_csv('updated_with_similar_codes_1.csv', index=False)

# Print before and after updates for review
print("=== Updates Made ===")
print(updates_df)
print("\n=== Updated DataFrame Preview ===")
print(df.head())

df

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Load the cleaned DataFrame
df = df_cleaned.copy()

# Reset index to ensure it is sequential
df = df.reset_index(drop=True)

# Initialize Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate sentence embeddings for the paragraphs
embeddings = model.encode(df['Paragraph'].tolist(), convert_to_tensor=False)

# Compute the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(embeddings)

# Define a similarity threshold (adjust based on the desired strictness)
similarity_threshold = 1

# Track changes in Final_Code
updates = []

# Function to assign a code based on similarity
def assign_similar_codes(row_idx, similarity_matrix, dataframe):
    original_code = dataframe.at[row_idx, 'Final_Code']
    similarities = similarity_matrix[row_idx]
    similar_indices = [i for i, score in enumerate(similarities) if score >= similarity_threshold and i != row_idx]

    # Look for a similar row's code
    for idx in similar_indices:
        if dataframe.at[idx, 'Final_Code']:
            new_code = dataframe.at[idx, 'Final_Code']
            if new_code != original_code:  # Update only if there's a change
                updates.append({
                    "Paragraph": dataframe.at[row_idx, 'Paragraph'],
                    "Original_Final_Code": original_code,
                    "Updated_Final_Code": new_code,
                    "Similar_Paragraph": dataframe.at[idx, 'Paragraph']
                })
            return new_code

    # Return the original code if no updates are made
    return original_code

# Apply the function to assign similar codes
df['Final_Code'] = [assign_similar_codes(idx, cosine_sim_matrix, df) for idx in range(len(df))]

# Convert updates to a DataFrame for review
updates_df = pd.DataFrame(updates)

# Save updates and final DataFrame
updates_df.to_csv('code_updates.csv', index=False)
df.to_csv('updated_with_similar_codes_1.csv', index=False)

# Print before and after updates for review
print("=== Updates Made ===")
print(updates_df)
print("\n=== Updated DataFrame Preview ===")
print(df.head())

!pip install rouge_score

similarity_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]

"""# Can start uploading consolidated file from here"""

import pandas as pd
import os

# Define the file path pattern and list all relevant files
file_paths = [
    '/content/updated_with_similar_codes_01.csv',
    '/content/updated_with_similar_codes_02.csv',
    '/content/updated_with_similar_codes_03.csv',
    '/content/updated_with_similar_codes_04.csv',
    '/content/updated_with_similar_codes_05.csv',
    '/content/updated_with_similar_codes_06.csv',
    '/content/updated_with_similar_codes_07.csv',
    '/content/updated_with_similar_codes_08.csv',
    '/content/updated_with_similar_codes_09.csv',
    '/content/updated_with_similar_codes_1.csv'
]

# Consolidate all the tables into a single DataFrame
consolidated_df = pd.DataFrame()

for file_path in file_paths:
    # Extract the "Final Code" column name based on file name
    file_name = os.path.basename(file_path)
    final_code_column = f"Final_Code {file_name.split('_')[-1].split('.')[0]}"

    # Read the current CSV
    current_df = pd.read_csv(file_path)

    # Rename the Final Code column to make it unique
    current_df.rename(columns={"Final_Code": final_code_column}, inplace=True)

    # Merge the data into the consolidated DataFrame
    if consolidated_df.empty:
        consolidated_df = current_df
    else:
        consolidated_df = pd.merge(
            consolidated_df, current_df[[final_code_column]], left_index=True, right_index=True
        )

# Save consolidated DataFrame
consolidated_df_path = "/content/consolidated_codes_table.csv"
consolidated_df.to_csv(consolidated_df_path, index=False)

consolidated_df_path

import pandas as pd
consolidated_df = pd.read_csv('/content/consolidated_codes_table (1).csv')

consolidated_df

#Here I will just use gpt4 to code

import pandas as pd
import random
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import openai  # Install the OpenAI library if not already installed



# Function to decide the final code
def gptcode(row):

        # Prepare the input prompt for GPT
        prompt = f"""
        You will be given a paragraph from the text.

        Definition of the code: A word or short phrase that symbolically assigns a summative, salient, essence-capturing, and/or evocative attribute for a portion of language-based or visual data.

        Here is the excerpt to code:
        {consolidated_df['Paragraph']}

        Please suggest a code for this excerpt.
        Output should be the code with no longer than 5 words.
        """
        # Call GPT chat-based API (replace with your API key)
        openai.api_key = ''
        response = openai.ChatCompletion.create(
            model="gpt-4o",  # Replace with the desired GPT model
            messages=[
                {"role": "system", "content": "You are a coding assistant skilled in thematic analysis."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200
        )

        # Extract the final decision from GPT's response
        return response['choices'][0]['message']['content'].strip()

# Apply the function to the dataset
consolidated_df['GPT4o_Code'] = consolidated_df.apply(gptcode, axis=1)

# Save the updated table
consolidated_df.to_csv('consolidated_codes_table_withgpt.csv', index=False)

print("Final codes have been decided and saved.")

consolidated_df

!pip install bert_score rouge_score

import numpy as np
from bert_score import score as bert_score
from rouge_score import rouge_scorer
import pandas as pd
from tqdm import tqdm

# Initialize result storage
results_gpt = []

# Loop through models
for model in ['GPT4o_Code']:
    bert_precision, bert_recall, bert_f1 = [], [], []
    rouge_1_scores, rouge_2_scores, rouge_l_scores = [], [], []

    # Loop through each row and compute scores
    for i, row in tqdm(consolidated_df.iterrows(), total=consolidated_df.shape[0]):
        ref = [row['GD']]
        hyp = [row[model]]

        # Calculate BERT scores
        p, r, f1 = bert_score(hyp, ref, lang='en', rescale_with_baseline=False)
        bert_precision.append(p.mean().item())
        bert_recall.append(r.mean().item())
        bert_f1.append(f1.mean().item())

        # Calculate ROUGE scores
        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        rouge_scores = rouge_scorer_instance.score(ref[0], hyp[0])
        rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)
        rouge_2_scores.append(rouge_scores['rouge2'].fmeasure)
        rouge_l_scores.append(rouge_scores['rougeL'].fmeasure)

    # Store results with means and standard deviations
    results_gpt.append({
        'Model': model,
        'BERT Precision Mean': np.mean(bert_precision),
        'BERT Precision Std': np.std(bert_precision),
        'BERT Recall Mean': np.mean(bert_recall),
        'BERT Recall Std': np.std(bert_recall),
        'BERT F1 Mean': np.mean(bert_f1),
        'BERT F1 Std': np.std(bert_f1),
        'ROUGE-1 Mean': np.mean(rouge_1_scores),
        'ROUGE-1 Std': np.std(rouge_1_scores),
        'ROUGE-2 Mean': np.mean(rouge_2_scores),
        'ROUGE-2 Std': np.std(rouge_2_scores),
        'ROUGE-L Mean': np.mean(rouge_l_scores),
        'ROUGE-L Std': np.std(rouge_l_scores)
    })

# Convert results to a DataFrame
results_gpt = pd.DataFrame(results_gpt)


# Display the results
print(results_gpt)

results_df = pd.read_csv('/content/evaluation_with_std_threshold.csv')

results_df

final_results = pd.concat([results_df, results_gpt], ignore_index=True)

final_results

final_results.to_csv('evaluation_with_std_threshold_withgpt.csv', index=False)

# Reimporting necessary libraries after reset
import pandas as pd
import matplotlib.pyplot as plt

# Data reconstruction
data = {
    "Model": [
        "Llama3_Code", "Falcon_Code", "Mistral_Code", "Final_Code 01",
        "Final_Code 02", "Final_Code 03", "Final_Code 04", "Final_Code 05",
        "Final_Code 06", "Final_Code 07", "Final_Code 08", "Final_Code 09", "Final_Code 1"
    ],
    "BERT Precision Mean": [0.840441, 0.82254, 0.816721, 0.8335, 0.836992, 0.840858, 0.841011, 0.841275, 0.840442, 0.841149, 0.840615, 0.840615, 0.840615],
    "BERT Precision Std": [0.044402, 0.02411, 0.018091, 0.021212, 0.021433, 0.020132, 0.029157, 0.028892, 0.028791, 0.028594, 0.028257, 0.028257, 0.028257],
    "BERT Recall Mean": [0.848797, 0.837142, 0.829195, 0.843191, 0.848411, 0.847896, 0.852731, 0.85473, 0.856563, 0.856996, 0.856625, 0.856625, 0.856625],
    "BERT Recall Std": [0.043206, 0.029338, 0.025497, 0.027303, 0.029515, 0.027314, 0.028113, 0.028549, 0.029832, 0.029284, 0.028992, 0.028992, 0.028992],
    "BERT F1 Mean": [0.844287, 0.829378, 0.82265, 0.838078, 0.842352, 0.844056, 0.846477, 0.847555, 0.848042, 0.848606, 0.848152, 0.848152, 0.848152],
    "BERT F1 Std": [0.040743, 0.019606, 0.016557, 0.019896, 0.019996, 0.01784, 0.022813, 0.022108, 0.023043, 0.022409, 0.021984, 0.021984, 0.021984],
    "ROUGE-1 Mean": [0.052554, 0.036884, 0.024115, 0.012658, 0.016275, 0.034629, 0.058077, 0.061634, 0.073659, 0.073659, 0.073659, 0.073659, 0.073659],
    "ROUGE-1 Std": [0.181967, 0.07795, 0.056301, 0.063712, 0.070625, 0.104502, 0.133353, 0.1418, 0.15943, 0.15943, 0.15943, 0.15943, 0.15943]
}

df = pd.DataFrame(data)

# Plotting
metrics = ["BERT Precision Mean", "BERT Recall Mean", "BERT F1 Mean", "ROUGE-1 Mean"]
std_metrics = ["BERT Precision Std", "BERT Recall Std", "BERT F1 Std", "ROUGE-1 Std"]

plt.figure(figsize=(12, 6))

for i, metric in enumerate(metrics):
    plt.errorbar(
        df["Model"], df[metric], yerr=df[std_metrics[i]],
        fmt='o-', label=f"{metric} ± Std"
    )

plt.xticks(rotation=45, ha='right')
plt.ylabel("Scores")
plt.title("Comparison of Models across Metrics with Standard Deviations")
plt.legend()
plt.tight_layout()
plt.show()





"""## Попытка заимплементить их аппроч"""

import numpy as np
import pandas as pd
from tqdm import tqdm
from scipy.spatial.distance import jensenshannon

# Initialize result storage
results = []

# Function to calculate metrics
def calculate_metrics(ref, hyp):
    # Example metrics: Replace with your specific metric calculations
    coverage = len(set(hyp).intersection(set(ref))) / len(set(ref)) if len(set(ref)) > 0 else 0
    density = len(hyp) / len(set(hyp)) if len(set(hyp)) > 0 else 0
    novelty = len(set(hyp).difference(set(ref))) / len(set(hyp)) if len(set(hyp)) > 0 else 0
    divergence = jensenshannon(
        np.array([1 if token in hyp else 0 for token in set(ref + hyp)]),
        np.array([1 if token in ref else 0 for token in set(ref + hyp)]),
        base=2
    )
    return coverage, density, novelty, divergence

# Loop through models
for model in ['Llama3_Code', 'Falcon_Code', 'Mistral_Code', 'Final_Code 01', 'GPT4o_Code', 'Final_Code 02', 'Final_Code 03', 'Final_Code 04',
              'Final_Code 05', 'Final_Code 06', 'Final_Code 07', 'Final_Code 08', 'Final_Code 09', 'Final_Code 1']:
    coverage_scores, density_scores, novelty_scores, divergence_scores = [], [], [], []

    # Loop through each row and compute scores
    for i, row in tqdm(consolidated_df.iterrows(), total=consolidated_df.shape[0]):
        ref = row['GD'].split()  # Assuming 'GD' column contains reference text
        hyp = row[model].split()  # Assuming model's column contains hypothesis text

        # Calculate metrics
        coverage, density, novelty, divergence = calculate_metrics(ref, hyp)
        coverage_scores.append(coverage)
        density_scores.append(density)
        novelty_scores.append(novelty)
        divergence_scores.append(divergence)

    # Store results with means and standard deviations
    results.append({
        'Model': model,
        'Coverage Mean': np.mean(coverage_scores),
        'Coverage Std': np.std(coverage_scores),
        'Density Mean': np.mean(density_scores),
        'Density Std': np.std(density_scores),
        'Novelty Mean': np.mean(novelty_scores),
        'Novelty Std': np.std(novelty_scores),
        'Divergence Mean': np.mean(divergence_scores),
        'Divergence Std': np.std(divergence_scores)
    })

# Convert results to a DataFrame
results_df = pd.DataFrame(results)
results_df.to_csv('evaluation_with_custom_metrics_withgpt.csv', index=False)

# Display the results
print(results_df)

import matplotlib.pyplot as plt
metrics = ["Coverage Mean", "Density Mean", "Novelty Mean", "Divergence Mean"]
std_metrics = ["Coverage Std", "Density Std", "Novelty Std", "Divergence Std"]

for metric, std_metric in zip(metrics, std_metrics):
    plt.figure(figsize=(12, 6))
    plt.bar(results_df["Model"], results_df[metric], yerr=results_df[std_metric], capsize=5, alpha=0.7, label=f"{metric} ± Std")
    plt.xlabel("Model")
    plt.ylabel(metric)
    plt.title(f"{metric} Across Models")
    plt.xticks(rotation=45, ha='right')
    plt.legend()
    plt.tight_layout()
    plt.show()

import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from scipy.spatial.distance import jensenshannon
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('wordnet')

# Initialize Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to calculate Jaccard Similarity
def jaccard_similarity(list1, list2):
    set1, set2 = set(list1), set(list2)
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0

# Initialize result storage
results = []

# Loop through models
for model_name in ['Llama3_Code', 'Falcon_Code', 'Mistral_Code', 'Final_Code 01', 'GPT4o_Code','Final_Code 02', 'Final_Code 03',
                   'Final_Code 04', 'Final_Code 05', 'Final_Code 06', 'Final_Code 07', 'Final_Code 08', 'Final_Code 09', 'Final_Code 1']:
    cosine_similarities = []
    jaccard_similarities = []
    bleu_scores = []
    meteor_scores_values = []
    js_divergences = []
    exact_matches = []

    # Loop through each row and compute scores
    for i, row in tqdm(consolidated_df.iterrows(), total=consolidated_df.shape[0]):
        ref = row['GD']
        hyp = row[model_name]

        # Tokenize the reference and hypothesis
        ref_tokens = ref.split()
        hyp_tokens = hyp.split()

        # Cosine Similarity
        ref_emb = model.encode([ref])
        hyp_emb = model.encode([hyp])
        cosine_similarities.append(cosine_similarity(ref_emb, hyp_emb)[0][0])

        # Jaccard Similarity
        jaccard_similarities.append(jaccard_similarity(ref_tokens, hyp_tokens))

        # BLEU Score
        bleu_scores.append(sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=SmoothingFunction().method1))

        # METEOR Score
        meteor_scores_values.append(meteor_score([ref_tokens], hyp_tokens))

        # Jensen-Shannon Divergence (using token distributions)
        vectorizer = CountVectorizer().fit([ref, hyp])
        ref_vec = vectorizer.transform([ref]).toarray()[0]
        hyp_vec = vectorizer.transform([hyp]).toarray()[0]
        ref_dist = ref_vec / ref_vec.sum() if ref_vec.sum() > 0 else ref_vec
        hyp_dist = hyp_vec / hyp_vec.sum() if hyp_vec.sum() > 0 else hyp_vec
        js_divergences.append(jensenshannon(ref_dist, hyp_dist) if np.any(hyp_dist) else np.nan)

        # Exact Match Ratio
        exact_matches.append(1 if ref == hyp else 0)

    # Store results with means and standard deviations
    results.append({
        'Model': model_name,
        'Cosine Similarity Mean': np.mean(cosine_similarities),
        'Cosine Similarity Std': np.std(cosine_similarities),
        'Jaccard Similarity Mean': np.mean(jaccard_similarities),
        'Jaccard Similarity Std': np.std(jaccard_similarities),
        'BLEU Score Mean': np.mean(bleu_scores),
        'BLEU Score Std': np.std(bleu_scores),
        'METEOR Score Mean': np.mean(meteor_scores_values),
        'METEOR Score Std': np.std(meteor_scores_values),
        'JS Divergence Mean': np.nanmean(js_divergences),
        'JS Divergence Std': np.nanstd(js_divergences),
        'Exact Match Ratio Mean': np.mean(exact_matches),
        'Exact Match Ratio Std': np.std(exact_matches)
    })

# Convert results to a DataFrame
results_df = pd.DataFrame(results)
results_df.to_csv('evaluation_with_additional_metrics_fixed_withgpt.csv', index=False)

# Display the results
print(results_df)

df = results_df

# Plot Cosine Similarity Mean
plt.figure(figsize=(10, 6))
plt.bar(df['Model'], df['Cosine Similarity Mean'], color='skyblue')
plt.title('Cosine Similarity Mean Across Models')
plt.xlabel('Model')
plt.ylabel('Cosine Similarity Mean')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Plot Jaccard Similarity Mean
plt.figure(figsize=(10, 6))
plt.bar(df['Model'], df['Jaccard Similarity Mean'], color='lightgreen')
plt.title('Jaccard Similarity Mean Across Models')
plt.xlabel('Model')
plt.ylabel('Jaccard Similarity Mean')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Plot BLEU Score Mean
plt.figure(figsize=(10, 6))
plt.bar(df['Model'], df['BLEU Score Mean'], color='orange')
plt.title('BLEU Score Mean Across Models')
plt.xlabel('Model')
plt.ylabel('BLEU Score Mean')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Plot METEOR Score Mean
plt.figure(figsize=(10, 6))
plt.bar(df['Model'], df['METEOR Score Mean'], color='purple')
plt.title('METEOR Score Mean Across Models')
plt.xlabel('Model')
plt.ylabel('METEOR Score Mean')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Plot JS Divergence Mean
plt.figure(figsize=(10, 6))
plt.bar(df['Model'], df['JS Divergence Mean'], color='red')
plt.title('JS Divergence Mean Across Models')
plt.xlabel('Model')
plt.ylabel('JS Divergence Mean')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Plot Exact Match Ratio Mean
plt.figure(figsize=(10, 6))
plt.bar(df['Model'], df['Exact Match Ratio Mean'], color='brown')
plt.title('Exact Match Ratio Mean Across Models')
plt.xlabel('Model')
plt.ylabel('Exact Match Ratio Mean')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

mean_code_lengths = []

# List of model column names
model_columns = ['Llama3_Code', 'Falcon_Code', 'Mistral_Code', 'GPT4o_Code','Final_Code 01',
                 'Final_Code 02', 'Final_Code 03', 'Final_Code 04', 'Final_Code 05',
                 'Final_Code 06', 'Final_Code 07', 'Final_Code 08', 'Final_Code 09',
                 'Final_Code 1']

# Calculate mean length of codes for each model
for model in model_columns:
    mean_length = consolidated_df[model].apply(lambda x: len(str(x).split())).mean()
    mean_code_lengths.append({'Model': model, 'Mean Code Length': mean_length})

# Convert to DataFrame for better visualization
mean_length_df = pd.DataFrame(mean_code_lengths)

mean_length_df



"""## Composite score computation"""

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.translate.meteor_score import meteor_score
from scipy.spatial.distance import jensenshannon
from tqdm import tqdm

# Initialize Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize result storage
results = []

# Loop through models
for model_name in ['Llama3_Code', 'Falcon_Code', 'Mistral_Code', 'Final_Code 01', 'GPT4o_Code', 'Final_Code 02',
                   'Final_Code 03', 'Final_Code 04', 'Final_Code 05', 'Final_Code 06', 'Final_Code 07',
                   'Final_Code 08', 'Final_Code 09', 'Final_Code 1']:
    cosine_similarities = []
    meteor_scores_values = []
    mean_code_lengths = []
    js_divergences = []

    # Loop through each row
    for _, row in tqdm(consolidated_df.iterrows(), total=consolidated_df.shape[0]):
        ref = row['GD']
        hyp = row[model_name]

        # Cosine Similarity
        embeddings = model.encode([ref, hyp], convert_to_tensor=False)
        cosine_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        cosine_similarities.append(cosine_sim)

        # METEOR Score
        meteor_scores_values.append(meteor_score([ref.split()], hyp.split()))

        # Mean Code Length
        mean_code_lengths.append(len(hyp.split()))

        # JS Divergence
        ref_tokens = ref.split()
        hyp_tokens = hyp.split()
        all_tokens = list(set(ref_tokens + hyp_tokens))
        ref_dist = [ref_tokens.count(token) / len(ref_tokens) for token in all_tokens]
        hyp_dist = [hyp_tokens.count(token) / len(hyp_tokens) for token in all_tokens]
        js_div = jensenshannon(ref_dist, hyp_dist, base=2)
        js_divergences.append(js_div)

    # Store results for the model
    results.append({
        'Model': model_name,
        'Cosine Similarity Mean': np.mean(cosine_similarities),
        'Cosine Similarity Std': np.std(cosine_similarities),
        'METEOR Score Mean': np.mean(meteor_scores_values),
        'METEOR Score Std': np.std(meteor_scores_values),
        'Mean Code Length': np.mean(mean_code_lengths),
        'Mean Code Length Std': np.std(mean_code_lengths),
        'JS Divergence Mean': np.mean(js_divergences),
        'JS Divergence Std': np.std(js_divergences)
    })

# Convert results to a DataFrame
results_df = pd.DataFrame(results)

# Save to file
results_df.to_csv('metrics_results_for composite_wihgpt.csv', index=False)

# Display results
print(results_df)

# Normalize function
def normalize(metric, min_val, max_val, invert=False):
    normalized = (metric - min_val) / (max_val - min_val)
    return 1 - normalized if invert else normalized

# Normalize metrics
results_df["Normalized Cosine Similarity"] = normalize(
    results_df["Cosine Similarity Mean"],
    results_df["Cosine Similarity Mean"].min(),
    results_df["Cosine Similarity Mean"].max()
)

results_df["Normalized METEOR"] = normalize(
    results_df["METEOR Score Mean"],
    results_df["METEOR Score Mean"].min(),
    results_df["METEOR Score Mean"].max()
)

results_df["Normalized Mean Code Length"] = normalize(
    results_df["Mean Code Length"],
    results_df["Mean Code Length"].min(),
    results_df["Mean Code Length"].max(),
    invert=True
)

results_df["Normalized JS Divergence"] = normalize(
    results_df["JS Divergence Mean"],
    results_df["JS Divergence Mean"].min(),
    results_df["JS Divergence Mean"].max(),
    invert=True
)

# Compute composite score
weights = {
    "Cosine Similarity": 0.25,
    "METEOR Score": 0.25,
    "Mean Code Length": 0.25,
    "JS Divergence": 0.25
}

results_df["Composite Score"] = (
    weights["Cosine Similarity"] * results_df["Normalized Cosine Similarity"] +
    weights["METEOR Score"] * results_df["Normalized METEOR"] +
    weights["Mean Code Length"] * results_df["Normalized Mean Code Length"] +
    weights["JS Divergence"] * results_df["Normalized JS Divergence"]
)

# Sort by Composite Score
results_df = results_df.sort_values("Composite Score", ascending=False)

# Save final results
results_df.to_csv('final_composite_results.csv', index=False)

# Display results
print(results_df)

import matplotlib.pyplot as plt


# Plot
plt.figure(figsize=(12, 6))
plt.bar(results_df['Model'], results_df['Composite Score'], color="skyblue")
plt.title("Composite Scores for Models (Equal Weights)", fontsize=14)
plt.xlabel("Models", fontsize=12)
plt.ylabel("Composite Score", fontsize=12)
plt.xticks(rotation=45, ha="right")
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()





